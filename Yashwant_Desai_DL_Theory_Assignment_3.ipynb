{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c27a818",
   "metadata": {},
   "source": [
    "# Yashwant Desai –  DL_Theory_Assignment_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3805f",
   "metadata": {},
   "source": [
    "# 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20834f",
   "metadata": {},
   "source": [
    "No, it's not advisable to initialize all the weights to the same value even with He initialization. The purpose of He initialization is to set the weights to random values drawn from a Gaussian distribution with a mean of 0 and a variance of 2/n where n is the number of input units. Initializing all weights to the same value would defeat the purpose of introducing randomness leading to symmetry problems during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d2492",
   "metadata": {},
   "source": [
    "# 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f913a7a",
   "metadata": {},
   "source": [
    "Yes, it is generally okay to initialize the bias terms to 0. Bias terms are used to shift the activation function, and setting them to 0 initially is a common practice. However there are variations of initialization techniques that involve non-zero bias initialization but it's not a requirement for most deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a399f60",
   "metadata": {},
   "source": [
    "# 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61516f73",
   "metadata": {},
   "source": [
    "Self-normalization: SELU is designed to maintain a mean output close to 0 and a standard deviation close to 1 during training which can help mitigate the vanishing/exploding gradient problem.\n",
    "\n",
    "Smoothness: SELU is smooth and differentiable everywhere which can lead to more stable and efficient training.\n",
    "\n",
    "Avoiding dead neurons: SELU can help prevent the issue of \"dying ReLU\" neurons by allowing some activation values to be negative which may lead to better information flow in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c91800",
   "metadata": {},
   "source": [
    "# 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0eca3e",
   "metadata": {},
   "source": [
    "SELU: Use SELU when you want self-normalization and a smooth, differentiable activation function, particularly in deep feedforward neural networks.\n",
    "\n",
    "Leaky ReLU and variants: Use them when you want to address the dying ReLU problem or want to introduce a small amount of non-linearity in the network.\n",
    "\n",
    "ReLU: It's a popular choice for most hidden layers in deep neural networks due to its simplicity and computational efficiency.\n",
    "\n",
    "tanh: Use tanh when you need centered activations that range from -1 to 1, such as in recurrent neural networks (RNNs).\n",
    "\n",
    "logistic (sigmoid): Use logistic activation in the output layer for binary classification problems.\n",
    "\n",
    "softmax: Use softmax activation in the output layer for multi-class classification problems to obtain class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a03ff4",
   "metadata": {},
   "source": [
    "# 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0de82",
   "metadata": {},
   "source": [
    "Setting the momentum hyperparameter too close to 1 can lead to slow convergence or even instability during training. A high momentum value means that the optimizer relies heavily on past gradients making it resistant to change in direction. This can cause the optimizer to overshoot the optimal solution and oscillate around it or diverge completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f4b2f",
   "metadata": {},
   "source": [
    "# 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e334a",
   "metadata": {},
   "source": [
    "L1 Regularization: By adding an L1 penalty term to the loss function, it encourages many model weights to be exactly zero creating a sparse model.\n",
    "\n",
    "Weight Pruning: Identify and set small-weight connections to zero after training, which sparsifies the model.\n",
    "\n",
    "Binary Connect: Train the model with binary weights (either -1 or 1) instead of real-valued weights, which results in a sparse binary network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a9e21",
   "metadata": {},
   "source": [
    "# 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb42660",
   "metadata": {},
   "source": [
    "Dropout can slow down training because it effectively reduces the capacity of the network requiring more training epochs. However dropout doesn't necessarily slow down inference significantly because during inference, dropout is turned off and the full model is used for predictions. MC Dropout, which involves running inference with dropout enabled multiple times and averaging the results can be slower but can provide better uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf2d45",
   "metadata": {},
   "source": [
    "# 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8d26bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEW\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 378s 2us/step\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 25s 12ms/step - loss: 1.8775 - accuracy: 0.3209 - val_loss: 1.7935 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6899 - accuracy: 0.3898 - val_loss: 1.6677 - val_accuracy: 0.3950\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6051 - accuracy: 0.4225 - val_loss: 1.5993 - val_accuracy: 0.4182\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5471 - accuracy: 0.4436 - val_loss: 1.6114 - val_accuracy: 0.4156\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5039 - accuracy: 0.4597 - val_loss: 1.5638 - val_accuracy: 0.4390\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4688 - accuracy: 0.4730 - val_loss: 1.5510 - val_accuracy: 0.4400\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4377 - accuracy: 0.4833 - val_loss: 1.5487 - val_accuracy: 0.4512\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 1.4094 - accuracy: 0.4952 - val_loss: 1.5616 - val_accuracy: 0.4420\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3837 - accuracy: 0.5027 - val_loss: 1.5228 - val_accuracy: 0.4628\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3559 - accuracy: 0.5122 - val_loss: 1.4807 - val_accuracy: 0.4686\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3338 - accuracy: 0.5215 - val_loss: 1.5045 - val_accuracy: 0.4630\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3086 - accuracy: 0.5310 - val_loss: 1.5338 - val_accuracy: 0.4660\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.2870 - accuracy: 0.5399 - val_loss: 1.5013 - val_accuracy: 0.4708\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2637 - accuracy: 0.5449 - val_loss: 1.5023 - val_accuracy: 0.4750\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2446 - accuracy: 0.5529 - val_loss: 1.4710 - val_accuracy: 0.4824\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2233 - accuracy: 0.5589 - val_loss: 1.4709 - val_accuracy: 0.4786\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2069 - accuracy: 0.5664 - val_loss: 1.4918 - val_accuracy: 0.4822\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.1857 - accuracy: 0.5717 - val_loss: 1.4889 - val_accuracy: 0.4844\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1686 - accuracy: 0.5792 - val_loss: 1.4678 - val_accuracy: 0.4946\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1493 - accuracy: 0.5862 - val_loss: 1.4952 - val_accuracy: 0.4866\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1324 - accuracy: 0.5934 - val_loss: 1.4892 - val_accuracy: 0.4928\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1151 - accuracy: 0.6007 - val_loss: 1.5503 - val_accuracy: 0.4764\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0966 - accuracy: 0.6070 - val_loss: 1.5667 - val_accuracy: 0.4738\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0771 - accuracy: 0.6126 - val_loss: 1.4952 - val_accuracy: 0.4880\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0625 - accuracy: 0.6189 - val_loss: 1.5167 - val_accuracy: 0.4924\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0447 - accuracy: 0.6238 - val_loss: 1.5681 - val_accuracy: 0.4764\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0317 - accuracy: 0.6286 - val_loss: 1.5447 - val_accuracy: 0.4758\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0063 - accuracy: 0.6384 - val_loss: 1.5515 - val_accuracy: 0.4992\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9946 - accuracy: 0.6407 - val_loss: 1.5054 - val_accuracy: 0.4954\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 43s 20ms/step - loss: 2.1001 - accuracy: 0.2610 - val_loss: 1.8802 - val_accuracy: 0.3352\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.8043 - accuracy: 0.3549 - val_loss: 1.8035 - val_accuracy: 0.3502\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.7183 - accuracy: 0.3878 - val_loss: 1.8608 - val_accuracy: 0.3572\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6560 - accuracy: 0.4120 - val_loss: 1.8617 - val_accuracy: 0.3576\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.6147 - accuracy: 0.4231 - val_loss: 1.6370 - val_accuracy: 0.4120\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.5697 - accuracy: 0.4410 - val_loss: 1.7100 - val_accuracy: 0.3992\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.5399 - accuracy: 0.4515 - val_loss: 1.5529 - val_accuracy: 0.4380\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.5104 - accuracy: 0.4628 - val_loss: 1.6453 - val_accuracy: 0.4196\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.4841 - accuracy: 0.4734 - val_loss: 1.5292 - val_accuracy: 0.4558\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4550 - accuracy: 0.4857 - val_loss: 1.6577 - val_accuracy: 0.4104\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.4387 - accuracy: 0.4910 - val_loss: 1.5305 - val_accuracy: 0.4580\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.4172 - accuracy: 0.4978 - val_loss: 1.5568 - val_accuracy: 0.4572\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.3997 - accuracy: 0.5043 - val_loss: 1.4471 - val_accuracy: 0.4778\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3782 - accuracy: 0.5122 - val_loss: 1.5903 - val_accuracy: 0.4468\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.3628 - accuracy: 0.5157 - val_loss: 1.5447 - val_accuracy: 0.4508\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3485 - accuracy: 0.5224 - val_loss: 1.5936 - val_accuracy: 0.4416\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.3319 - accuracy: 0.5277 - val_loss: 1.4750 - val_accuracy: 0.4756\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 37s 26ms/step - loss: 1.3153 - accuracy: 0.5333 - val_loss: 1.4979 - val_accuracy: 0.4734\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.3048 - accuracy: 0.5373 - val_loss: 1.5331 - val_accuracy: 0.4612\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.2878 - accuracy: 0.5440 - val_loss: 1.4610 - val_accuracy: 0.4844\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.2789 - accuracy: 0.5465 - val_loss: 1.4557 - val_accuracy: 0.4830\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.2680 - accuracy: 0.5492 - val_loss: 1.4540 - val_accuracy: 0.4876\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.2560 - accuracy: 0.5559 - val_loss: 1.5035 - val_accuracy: 0.4694\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 25s 14ms/step - loss: 1.8813 - accuracy: 0.3178 - val_loss: 1.8384 - val_accuracy: 0.3428\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6914 - accuracy: 0.3894 - val_loss: 1.6905 - val_accuracy: 0.3954\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.6089 - accuracy: 0.4194 - val_loss: 1.6608 - val_accuracy: 0.4072\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5646 - accuracy: 0.4366 - val_loss: 1.6027 - val_accuracy: 0.4222\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5223 - accuracy: 0.4513 - val_loss: 1.5749 - val_accuracy: 0.4286\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4827 - accuracy: 0.4680 - val_loss: 1.5493 - val_accuracy: 0.4520\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4572 - accuracy: 0.4770 - val_loss: 1.5449 - val_accuracy: 0.4450\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.4294 - accuracy: 0.4883 - val_loss: 1.5307 - val_accuracy: 0.4508\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4080 - accuracy: 0.4958 - val_loss: 1.5008 - val_accuracy: 0.4686\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.3853 - accuracy: 0.5048 - val_loss: 1.4967 - val_accuracy: 0.4612\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.3658 - accuracy: 0.5104 - val_loss: 1.4853 - val_accuracy: 0.4732\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.3441 - accuracy: 0.5174 - val_loss: 1.5153 - val_accuracy: 0.4636\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3223 - accuracy: 0.5279 - val_loss: 1.4809 - val_accuracy: 0.4720\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.3073 - accuracy: 0.5303 - val_loss: 1.4802 - val_accuracy: 0.4756\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2869 - accuracy: 0.5362 - val_loss: 1.5238 - val_accuracy: 0.4656\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2691 - accuracy: 0.5450 - val_loss: 1.4998 - val_accuracy: 0.4704\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2557 - accuracy: 0.5526 - val_loss: 1.4589 - val_accuracy: 0.4768\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2382 - accuracy: 0.5572 - val_loss: 1.5044 - val_accuracy: 0.4704\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2203 - accuracy: 0.5630 - val_loss: 1.4707 - val_accuracy: 0.4814\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2050 - accuracy: 0.5669 - val_loss: 1.4548 - val_accuracy: 0.4846\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 837s 595ms/step - loss: 1.1841 - accuracy: 0.5740 - val_loss: 1.5037 - val_accuracy: 0.4756\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1664 - accuracy: 0.5796 - val_loss: 1.4814 - val_accuracy: 0.4858\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1552 - accuracy: 0.5812 - val_loss: 1.4884 - val_accuracy: 0.4890\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1403 - accuracy: 0.5896 - val_loss: 1.4698 - val_accuracy: 0.4884\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1239 - accuracy: 0.5943 - val_loss: 1.4843 - val_accuracy: 0.4834\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1064 - accuracy: 0.6009 - val_loss: 1.5377 - val_accuracy: 0.4806\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.0937 - accuracy: 0.6081 - val_loss: 1.5471 - val_accuracy: 0.4744\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0785 - accuracy: 0.6103 - val_loss: 1.5072 - val_accuracy: 0.4836\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0623 - accuracy: 0.6159 - val_loss: 1.4722 - val_accuracy: 0.5024\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.0516 - accuracy: 0.6201 - val_loss: 1.5165 - val_accuracy: 0.4862\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 36s 21ms/step - loss: 2.8604 - accuracy: 0.1016 - val_loss: 2.5185 - val_accuracy: 0.0964\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 2.6474 - accuracy: 0.0970 - val_loss: 2.3713 - val_accuracy: 0.0996\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 2.5172 - accuracy: 0.1002 - val_loss: 2.3268 - val_accuracy: 0.1022\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 2.4397 - accuracy: 0.0994 - val_loss: 2.3448 - val_accuracy: 0.1018\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 2.3879 - accuracy: 0.0990 - val_loss: 2.3312 - val_accuracy: 0.1048\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 2.3561 - accuracy: 0.0988 - val_loss: 2.3221 - val_accuracy: 0.1018\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 2.3330 - accuracy: 0.1007 - val_loss: 2.3121 - val_accuracy: 0.1018\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 2.3201 - accuracy: 0.1009 - val_loss: 2.3162 - val_accuracy: 0.1008\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 2.3126 - accuracy: 0.1004 - val_loss: 2.3174 - val_accuracy: 0.0988\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 2.3077 - accuracy: 0.1002 - val_loss: 2.3130 - val_accuracy: 0.1008\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 2.3068 - accuracy: 0.0982 - val_loss: 2.3064 - val_accuracy: 0.0970\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 2.3053 - accuracy: 0.0993 - val_loss: 2.3103 - val_accuracy: 0.1000\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 2.3046 - accuracy: 0.1010 - val_loss: 2.3108 - val_accuracy: 0.0970\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 2.3042 - accuracy: 0.1002 - val_loss: 2.3125 - val_accuracy: 0.1054\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 2.3045 - accuracy: 0.0983 - val_loss: 2.3108 - val_accuracy: 0.0970\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 2.3038 - accuracy: 0.1024 - val_loss: 2.3185 - val_accuracy: 0.1018\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 2.3042 - accuracy: 0.0989 - val_loss: 2.3066 - val_accuracy: 0.0858\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 2.3043 - accuracy: 0.0986 - val_loss: 2.3066 - val_accuracy: 0.1018\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 2.3040 - accuracy: 0.0996 - val_loss: 2.3050 - val_accuracy: 0.1018\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 2.3037 - accuracy: 0.1006 - val_loss: 2.3085 - val_accuracy: 0.0974\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 2.3042 - accuracy: 0.0958 - val_loss: 2.3087 - val_accuracy: 0.0970\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 2.3039 - accuracy: 0.0978 - val_loss: 2.3096 - val_accuracy: 0.0970\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 2.3039 - accuracy: 0.0996 - val_loss: 2.3128 - val_accuracy: 0.0970\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 2.3038 - accuracy: 0.0995 - val_loss: 2.3128 - val_accuracy: 0.0974\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 2.3039 - accuracy: 0.0995 - val_loss: 2.3078 - val_accuracy: 0.0974\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 2.3040 - accuracy: 0.1008 - val_loss: 2.3115 - val_accuracy: 0.1046\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 2.3038 - accuracy: 0.0970 - val_loss: 4.5929 - val_accuracy: 0.1030\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 21s 15ms/step - loss: 2.3038 - accuracy: 0.0994 - val_loss: 5.9371 - val_accuracy: 0.0812\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 2.3039 - accuracy: 0.0989 - val_loss: 4.1882 - val_accuracy: 0.1010\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, ELU\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import BatchNormalization, AlphaDropout\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "\n",
    "# Step a: Build a DNN with 20 hidden layers of 100 neurons each using He initialization and ELU activation.\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(32*32*3,)))\n",
    "for _ in range(19):\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Step b: Load and preprocess the CIFAR-10 dataset.\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_full = X_train_full.reshape(X_train_full.shape[0], -1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).astype('float32') / 255.0\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "\n",
    "# Compile the model and configure early stopping.\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])\n",
    "\n",
    "# Step c: Add Batch Normalization and compare the learning curves.\n",
    "model_with_bn = Sequential()\n",
    "model_with_bn.add(Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(32*32*3,)))\n",
    "model_with_bn.add(BatchNormalization())\n",
    "for _ in range(19):\n",
    "    model_with_bn.add(Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model_with_bn.add(BatchNormalization())\n",
    "\n",
    "model_with_bn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_with_bn.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "history_with_bn = model_with_bn.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])\n",
    "\n",
    "# Step d: Replace Batch Normalization with SELU and make necessary adjustments.\n",
    "model_with_selu = Sequential()\n",
    "model_with_selu.add(Dense(100, activation='selu', kernel_initializer=lecun_normal(), input_shape=(32*32*3,)))\n",
    "for _ in range(19):\n",
    "    model_with_selu.add(Dense(100, activation='selu', kernel_initializer=lecun_normal()))\n",
    "\n",
    "model_with_selu.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_with_selu.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "history_with_selu = model_with_selu.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])\n",
    "\n",
    "# Step e: Regularize the model with alpha dropout and experiment with MC Dropout.\n",
    "model_with_alpha_dropout = Sequential()\n",
    "model_with_alpha_dropout.add(Dense(100, activation='selu', kernel_initializer=lecun_normal(), input_shape=(32*32*3,)))\n",
    "for _ in range(19):\n",
    "    model_with_alpha_dropout.add(Dense(100, activation='selu', kernel_initializer=lecun_normal()))\n",
    "    model_with_alpha_dropout.add(AlphaDropout(0.5))  # Alpha dropout with a rate of 0.5\n",
    "\n",
    "model_with_alpha_dropout.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_with_alpha_dropout.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "history_with_alpha_dropout = model_with_alpha_dropout.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])\n",
    "\n",
    "# MC Dropout: Use a function to make predictions with dropout enabled, and run it multiple times to average predictions.\n",
    "def predict_with_mc_dropout(model, X, n_iterations=100):\n",
    "    result = np.stack([model.predict(X) for _ in range(n_iterations)])\n",
    "    return result.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7d5b1",
   "metadata": {},
   "source": [
    "# Done all 8 questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04469f9",
   "metadata": {},
   "source": [
    "# Regards,Yashwant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
